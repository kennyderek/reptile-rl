{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "<ul>\n",
    "    <li> Choose the world environment, and define a sampler function that generates random initializations of this environment.\n",
    "    <li> Set up REINFORCE and define model arguments.\n",
    "    <li> Train REINFORCE on an example instance of the world you set up in step one.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the world environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sim import SideScroller, Gobble, NoGobble, RockOn\n",
    "from sim import MazeArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "\n",
    "def sample_gobble():\n",
    "    args = MazeArgs()\n",
    "    args.rows = 6\n",
    "    args.cols = 6\n",
    "    args.targets = [[randint(0, args.rows-1), randint(0, args.cols-1)] for i in range(randint(1, 2))]\n",
    "    return Gobble(args)\n",
    "\n",
    "def sample_no_gobble():\n",
    "    args = MazeArgs()\n",
    "    args.rows = 6\n",
    "    args.cols = 6\n",
    "    args.targets = [[randint(0, args.rows-1), randint(0, args.cols-1)] for i in range(randint(1, 2))]\n",
    "    return NoGobble(args)\n",
    "\n",
    "def sample_scroller():\n",
    "    args = MazeArgs()\n",
    "    args.rows = 6\n",
    "    args.cols = 6\n",
    "    args.blockers = []\n",
    "    for i in range(0, 2):\n",
    "        x_loc = randint(1, args.cols - 2)\n",
    "        wall_type = randint(0, 3)\n",
    "        if wall_type == 0:\n",
    "            args.blockers.append([x_loc, args.rows - 1])\n",
    "            args.blockers.append([x_loc, args.rows - 2])\n",
    "        elif wall_type == 1:\n",
    "            args.blockers.append([x_loc, args.rows - 1])\n",
    "        elif wall_type == 2:\n",
    "            args.blockers.append([x_loc, args.rows - 2])\n",
    "    return SideScroller(args)\n",
    "\n",
    "def sample_task_named():\n",
    "    which = randint(0, 2)\n",
    "    if which == 1:\n",
    "        return \"gobble\", sample_gobble()\n",
    "    elif which == 2:\n",
    "        return \"no gobble\", sample_no_gobble()\n",
    "    elif which == 0:\n",
    "        return \"scroller\", sample_scroller()\n",
    "\n",
    "def sample_rock():\n",
    "    args = MazeArgs()\n",
    "    args.rows = 6\n",
    "    args.cols = 6\n",
    "    args.num_rocks = 2\n",
    "    return RockOn(args)\n",
    "    \n",
    "def sample_task():\n",
    "    return sample_task_named()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view sample environments\n",
    "for i in range(0, 5):\n",
    "    print(\"***\")\n",
    "    name, env = sample_task_named()\n",
    "    print(name)\n",
    "    env.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reinforce import REINFORCE\n",
    "import utils_training\n",
    "\n",
    "from utils import ActorSmall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelArgs():   \n",
    "    def __init__(self, world):\n",
    "        # type of model related arguments\n",
    "        self.seed = 1\n",
    "        self.state_input_size = world.state_size\n",
    "        self.action_space_size = world.num_actions\n",
    "        self.lr = 3e-4\n",
    "        self.ppo = True\n",
    "        self.ppo_base_epsilon = 0.2\n",
    "        self.ppo_dec_epsilon = 0.0\n",
    "        self.use_critic = True\n",
    "        self.use_entropy = False\n",
    "\n",
    "        # training related arguments\n",
    "        self.gradient_clipping = True\n",
    "        self.random_perm = True\n",
    "        self.num_batches = 300\n",
    "        self.num_mini_batches = 1\n",
    "        self.batch_size = 5\n",
    "        self.horizon = 100\n",
    "        self.weight_func = lambda batch_num: (1 - batch_num/self.num_batches)**2\n",
    "        \n",
    "        # policy\n",
    "        self.policy = ActorSmall\n",
    "        self.log_goal_locs = False\n",
    "        self.hidden_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on each of the sample worlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "QUICK_RUN = True # set to false to train on individual envs\n",
    "num_batches_each = 2 # should be increased to 100 to do actual training\n",
    "if not QUICK_RUN:\n",
    "    model_args = ModelArgs(sample_gobble())\n",
    "    model_args.log_goal_locs = False\n",
    "    model_args.num_batches = num_batches_each\n",
    "    model_args.batch_size = 3\n",
    "    model_args.num_mini_batches = 1\n",
    "    model = REINFORCE(model_args)\n",
    "    rewards, losses = model.train(sample_gobble().generate_fresh())\n",
    "    utils_training.plot_rewards(rewards, folder=None)\n",
    "\n",
    "    model_args = ModelArgs(sample_gobble())\n",
    "    model_args.log_goal_locs = False\n",
    "    model_args.num_batches = num_batches_each\n",
    "    model_args.batch_size = 3\n",
    "    model_args.num_mini_batches = 1\n",
    "    model = REINFORCE(model_args)\n",
    "    rewards, losses = model.train(sample_no_gobble().generate_fresh())\n",
    "    utils_training.plot_rewards(rewards, folder=None)\n",
    "\n",
    "    model_args = ModelArgs(sample_rock())\n",
    "    model_args.log_goal_locs = False\n",
    "    model_args.num_batches = num_batches_each\n",
    "    model_args.batch_size = 3\n",
    "    model_args.num_mini_batches = 1\n",
    "    model = REINFORCE(model_args)\n",
    "    rewards, losses = model.train(sample_rock().generate_fresh())\n",
    "    utils_training.plot_rewards(rewards, folder=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare different initializations from fast adaptation algorithms\n",
    "<ul>\n",
    "    <li> Batch REPTILE\n",
    "    <li> Batch pretraining on samples of environment\n",
    "    <li> Random initialization\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with each method, we add the method's resulting parameters to PARAMS_LIST\n",
    "PARAMS_LIST = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch REPTILE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils_training import update_init_params\n",
    "from tqdm.notebook import tqdm\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "NUM_META_ITER = 500\n",
    "NUM_TASKS = 10\n",
    "K = 4\n",
    "ALPHA = 0.1\n",
    "\n",
    "# random initialization\n",
    "model_args = ModelArgs(Gobble)\n",
    "model_args.num_batches = K\n",
    "model_args.batch_size = 5\n",
    "model = REINFORCE(model_args)\n",
    "\n",
    "for i in tqdm(range(NUM_META_ITER)):\n",
    "    tasks = [sample_task() for _ in range(NUM_TASKS)]\n",
    "    \n",
    "    init_params = copy.deepcopy(OrderedDict(model.policy.named_parameters()))\n",
    "    temp_params = copy.deepcopy(OrderedDict(model.policy.named_parameters()))\n",
    "\n",
    "    for t in tasks:\n",
    "        model.policy.load_state_dict(init_params)\n",
    "        model.init_optimizers()\n",
    "\n",
    "        model.train(t)\n",
    "        target_policy = OrderedDict(model.policy.named_parameters())\n",
    "\n",
    "        temp_params = update_init_params(target_policy, temp_params, ALPHA/K)\n",
    "    \n",
    "    model.policy.load_state_dict(temp_params)\n",
    "\n",
    "    \n",
    "result = {\n",
    "        \"pi\": model.state_dict(),\n",
    "        \"label\": \"REPTILE\"\n",
    "    }\n",
    "PARAMS_LIST.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_args = ModelArgs(sample_task())\n",
    "model_args.num_batches = 500\n",
    "model_args.batch_size = 10\n",
    "model = REINFORCE(model_args)\n",
    "\n",
    "# we pass the model no enviornment, but just the sampler.\n",
    "rewards, losses = model.train(None, sample_task)\n",
    "\n",
    "result = {\n",
    "        \"pi\": model.state_dict(),\n",
    "        \"label\": \"PRETRAIN\"\n",
    "    }\n",
    "PARAMS_LIST.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArgs(sample_task())\n",
    "model_args.num_batches = 20\n",
    "model_args.batch_size = 5\n",
    "model = REINFORCE(model_args)\n",
    "\n",
    "result = {\n",
    "        \"pi\": model.state_dict(),\n",
    "        \"label\": \"RANDOM\"\n",
    "    }\n",
    "PARAMS_LIST.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the adaptation speed of initial parameters\n",
    "\n",
    "Use the stored parameters in PARAMS_LIST to establish confidence intervals for their adaptation performance on new tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_GRAD_UPDATES = 4\n",
    "NUM_TEST_TASKS = 5\n",
    "\n",
    "model_args.num_batches = NUM_GRAD_UPDATES\n",
    "model_args.batch_size = 1\n",
    "model_args.num_mini_batches = 1\n",
    "utils_training.compare_parameter_initializations(PARAMS_LIST, model_args, NUM_TEST_TASKS, sample_task)\n",
    "utils_training.plot_adaptation(PARAMS_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
